import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
print('âš™ï¸  torth ì„¤ì • ì™„ë£Œ')
from sklearn.metrics import average_precision_score
import numpy as np
from tqdm import tqdm
import collections
from torch.utils.data.sampler import Sampler
import matplotlib.pyplot as plt
from public_function import *
from torch.utils.data import DataLoader, TensorDataset
from PIL import Image
print('âš™ï¸  í™˜ê²½ì„¤ì • ì™„ë£Œ')
import streamlit as st

if 'transform' not in st.session_state:
    st.session_state.transform = transforms.Compose([
        transforms.Resize((512, 512)),  # ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 512x512ë¡œ ê³ ì • ğŸ“
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])
    ])    
if 'train_dataset' not in st.session_state:
    st.session_state.train_dataset = torchvision.datasets.ImageFolder(root='my_dataset', transform=st.session_state.transform)
if 'sampler' not in st.session_state:
    st.session_state.sampler = MPerClassSampler(st.session_state.train_dataset.targets, m=4, length_before_new_iter=len(st.session_state.train_dataset))
if 'train_dataloader' not in st.session_state:
    st.session_state.train_dataloader = torch.utils.data.DataLoader(
        st.session_state.train_dataset, batch_size=44, shuffle=False, sampler=st.session_state.sampler
    )
if 'device' not in st.session_state:
    st.session_state.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if 'model' not in st.session_state:
    # ê¸°ë³¸ì ì¸ ëª¨ë¸
    class ResNet18FeatureExtractor(nn.Module):
        def __init__(self):
            super(ResNet18FeatureExtractor, self).__init__()
            resnet = torchvision.models.resnet18(pretrained=True)
            self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])

        def forward(self, x):
            x = self.feature_extractor(x)
            return x.view(x.size(0), -1)
    st.session_state.model = ResNet18FeatureExtractor()
    st.session_state.model.load_state_dict(torch.load("trained_model.pth", map_location=st.session_state.device))
    st.session_state.model = st.session_state.model.to(st.session_state.device)
    st.session_state.model.eval()
    

def extract(model, dataloader):
    features = []
    labels = []
    with torch.no_grad():
        for inputs, targets in tqdm(dataloader, desc="íŠ¹ì§• ì¶”ì¶œ"):
            # torchì— ë„£ê¸°
            inputs = inputs.to(st.session_state.device)
            outputs = model(inputs)
            
            # cpuë¡œ ì „í™˜ (cpuê°€ ë” ë¹ ë¦„)
            features.append(outputs.cpu().numpy())
            labels.append(targets.cpu().numpy())
            
    # ë°˜í™˜
    return np.concatenate(features), np.concatenate(labels)    

if 'feature_extract' not in st.session_state:
    st.session_state.feature_extract, st.session_state.label = extract(st.session_state.model, st.session_state.train_dataloader)
    
    
def extract_one(image):
    image = Image.open(image)
    image = image.convert('RGB')
    x = st.session_state.transform(image).unsqueeze(0)
    dummy_label = torch.tensor([0])

    dataset = TensorDataset(x, dummy_label)
    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)

    feature, label = extract(st.session_state.model, dataloader)
    return feature, label

# ìœ í´ë¦¬ë“œ ê±°ë¦¬
def euclidean_distance(query_feature, database_features):
    distances = np.linalg.norm(database_features - query_feature, axis=1)
    return distances

# ê²€ìƒ‰ ê²°ê³¼ ì–»ê¸°
def get_top_k_results(query, features, labels, k=5):
    distances = euclidean_distance(query, features)
    indices = np.argsort(distances)[:k]
    return indices, labels[indices]


# ì´ë¯¸ì§€ ì—…ë¡œë“œ ë°›ê¸°
uploaded_file = st.file_uploader("", type=["jpg", "jpeg", "png"])
if uploaded_file is not None:
    feature, _ = extract_one(uploaded_file)
    idx, labels = get_top_k_results(feature,  st.session_state.feature_extract,  st.session_state.label, k=15)
    for i in idx:
        path, label_index =  st.session_state.train_dataset.samples[i]
        label_name =  st.session_state.train_dataset.classes[label_index]  # ë¬¸ì ë¼ë²¨ëª…
        st.write(f'### ğŸ”–ë¼ë²¨: {label_name}')
        st.image(path)